{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pillasaikumar/Fmml/blob/main/FMML_Module_2_project_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2: Appreciating, Interpreting and Visualizing Data\n",
        "## Project\n",
        "\n",
        "```\n",
        "Coordinator: Aswin Jose\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "R976BTHqth_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "PjKikpD7uCnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we wil be performing a simple Exploratory Data Anaysis for this project. We will use the methods we learned in the tutorials to have a basic understanding of the dataset. So first we will start with the heart dataset available from kaggle. the infomration about the columns of the dataset is given below:    \n",
        "-age    \n",
        "-sex    \n",
        "-chest pain type (4 values)    \n",
        "-resting blood pressure    \n",
        "-serum cholestoral in mg/dl    \n",
        "-fasting blood sugar > 120 mg/dl    \n",
        "-resting electrocardiographic results (values 0,1,2)    \n",
        "-maximum heart rate achieved    \n",
        "-exercise induced angina   \n",
        "-oldpeak = ST depression induced by exercise relative to rest    \n",
        "-the slope of the peak exercise ST segment    \n",
        "-number of major vessels (0-3) colored by flourosopy    \n",
        "-:thal: 0 = normal; 1 = fixed defect; 2 = reversable defect    "
      ],
      "metadata": {
        "id": "Ep9v4Rj1rDaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in the portions that says \"to do\""
      ],
      "metadata": {
        "id": "GVPNEAfBrs7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded1 = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "I9sX_JSdtpsH",
        "outputId": "193e64cf-a9a7-4787-e4d8-67ae75ea96f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-45069a39-c40c-4c36-b185-6cbe7ffabad8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-45069a39-c40c-4c36-b185-6cbe7ffabad8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"heart.csv\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "7CUciJW-t-aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "j3fHpCWfuWSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns\n"
      ],
      "metadata": {
        "id": "DuxMngbivcdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preparation"
      ],
      "metadata": {
        "id": "N1OCngCVr5H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## we will be comparing rest of the parameters/columns present in the data with respect to precence or absece of heart disease\n",
        "data['target'] = data.target.replace({1: \"Disease\", 0: \"No_disease\"})\n",
        "data['sex'] = data.sex.replace({1: \"Male\", 0: \"Female\"})\n",
        "data['cp'] = data.cp.replace({1: \"typical_angina\",\n",
        "                          2: \"atypical_angina\",\n",
        "                          3:\"non-anginal pain\",\n",
        "                          4: \"asymtomatic\"})\n",
        "data['exang'] = data.exang.replace({1: \"Yes\", 0: \"No\"})\n",
        "data['slope'] = data.cp.replace({1: \"upsloping\",\n",
        "                          2: \"flat\",\n",
        "                          3:\"downsloping\"})\n",
        "data['thal'] = data.thal.replace({1: \"fixed_defect\", 2: \"reversable_defect\", 3:\"normal\"})"
      ],
      "metadata": {
        "id": "fRHrjko7xIwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "wl5oM9BZ2XVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, lets look at the difference in the number of samples with and without disease using a barplot."
      ],
      "metadata": {
        "id": "RKjU-NUW3op-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data['target'].value_counts())\n",
        "plt.title('Heart Disease Classes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9H3aQvCe2prB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## we can plot the same barplots usng the pandas inbuilt plotting functions.\n",
        "data['target'].value_counts().plot(kind='bar').set_title('Heart Disease Classes')"
      ],
      "metadata": {
        "id": "UpL7yRHR2ZA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Now plot a barplot indicating the the sex of the participants involved in the study, use whatever method of ploting comfortable for you\n",
        "## to do"
      ],
      "metadata": {
        "id": "J3pWoE-j282Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## pie charts can also be used to show the same infomation in a different manner\n",
        "plt.pie(data['target'].value_counts(), labels=[\"Disease\", \"No disease\"], autopct='%1.1f%%')\n",
        "plt.title('Target Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6lzeMeXx6Ddl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# next we will plot the counts of all the non-continous features present in the dataset.\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(17,10))\n",
        "cat_feat = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
        "\n",
        "for idx, feature in enumerate(cat_feat):\n",
        "    ax = axes[int(idx/3), idx%3]\n",
        "    sns.barplot(data[feature].value_counts(), ax=ax)"
      ],
      "metadata": {
        "id": "lhrwKDM62l67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  now lets play with 2 vaiables in dataset. Lets see if chest pain translates to the presence of desease in most cases...\n",
        "sns.countplot(x='cp', hue='target', data=data, palette='rainbow').set_title('Disease classes according to Chest Pain')"
      ],
      "metadata": {
        "id": "5MNCxVhQ4x30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets visualise count of all vairables w.r.t the presence of disease togather:\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(17,10))\n",
        "cat_feat = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
        "\n",
        "for idx, feature in enumerate(cat_feat):\n",
        "    ax = axes[int(idx/3), idx%3]\n",
        "    ## to do\n"
      ],
      "metadata": {
        "id": "smGhn01t5bqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualising the distribution of the continous variables"
      ],
      "metadata": {
        "id": "V0CiDMIK6pCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## pair plots can automoaticaly be used to viwe the pairwise relationship between all the  feature that we selected\n",
        "continous_features = ['age', 'chol', 'thalach', 'oldpeak','trestbps']\n",
        "sns.pairplot(data[continous_features + ['target']], hue='target')"
      ],
      "metadata": {
        "id": "6KnGCZz632hG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets try to understand the relationship between age and chol in each of the target based on sex.\n",
        "sns.lmplot(x=\"age\", y=\"chol\", hue=\"sex\", col=\"target\",\n",
        "           palette=\"Set1\",\n",
        "           data=data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DtjSCsgl6vV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_data = data[continous_features]\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr_matrix = numeric_data.corr()\n",
        "sns.heatmap(corr_matrix, annot=True)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ywPbmTHx8Rqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, len(continous_features), figsize=(25, 4), sharex=False, sharey=False)\n",
        "\n",
        "for idx, feature in enumerate(continous_features):\n",
        "    sns.boxplot(x='target', y=feature, data=data, ax=axes[idx])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nOQvj2UT-4Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: plot the cumulative variace of pca for all the possibel pronviopal components\n",
        "## to do\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "pca = PCA()\n",
        "pca.fit(numeric_data)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QLy3U9yOAAa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca.fit(numeric_data)\n",
        "pca_data = pca.transform(numeric_data)\n",
        "\n",
        "# Create a DataFrame with the principal components and target labels\n",
        "pca_df = pd.DataFrame({\n",
        "    \"pca_1\": pca_data[:, 0],\n",
        "    \"pca_2\": pca_data[:, 1],\n",
        "    \"target\": data[\"target\"]\n",
        "})\n",
        "\n",
        "# Visualize the PCA results with a scatter plot\n",
        "sns.scatterplot(x=\"pca_1\", y=\"pca_2\", hue=\"target\", data=pca_df)\n",
        "plt.title(\"PCA Visualization of Heart Disease Data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9kb_TztC9hqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Initialize and fit the TSNE model\n",
        "tsne = TSNE(n_components=2)\n",
        "tsne_data = tsne.fit_transform(numeric_data)\n",
        "\n",
        "# Create a DataFrame with the TSNE components and target labels\n",
        "tsne_df = pd.DataFrame({\n",
        "    \"tsne_1\": tsne_data[:, 0],\n",
        "    \"tsne_2\": tsne_data[:, 1],\n",
        "    \"target\": data[\"target\"]\n",
        "})\n",
        "\n",
        "# Visualize the TSNE results with a scatter plot\n",
        "sns.scatterplot(x=\"tsne_1\", y=\"tsne_2\", hue=\"target\", data=tsne_df)\n",
        "plt.title(\"TSNE Visualization of Heart Disease Data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QDKt3GQIAMHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the plots above, answer the following questions:    \n",
        "1. What is the percentage of Samples with Disease?    \n",
        "2. what are the 3 continous features that shows a singnficanct statistical differnce in distribution with respect to the precence and absence of the disease?    \n",
        "3. Can we see a clear seperation in terms of the presence/absence of disease in the features obtained from pca and tsne plots?    \n",
        "4. What is the optimal number of principal components in our case?    \n",
        "5. what are the continous features with the highest correation with each other?"
      ],
      "metadata": {
        "id": "AoQ8InmSuetP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.To identify continuous features with the highest correlation with each other, you can calculate the correlation matrix of the dataset. Here's how it's done:\n",
        "\n",
        "1. Calculate the Correlation Matrix:\n",
        "The correlation matrix shows the pairwise correlation coefficients between all continuous features. The most common correlation measure is Pearson‚Äôs correlation, which assesses the linear relationship between two variables.\n",
        "A correlation coefficient (r) ranges from:\n",
        "+1: Perfect positive correlation (as one feature increases, the other increases).\n",
        "-1: Perfect negative correlation (as one feature increases, the other decreases).\n",
        "0: No linear relationship.\n",
        "2. Steps:\n",
        "Compute the Pearson correlation coefficient for each pair of continuous features.\n",
        "Extract the pairs with the highest positive or negative correlation (closest to +1 or -1).\n",
        "3. Visualizing the Correlation Matrix:\n",
        "Use a heatmap to visualize the correlation matrix. This helps identify pairs of features with strong correlations more easily.\n",
        "Example Workflow:\n",
        "Generate the correlation matrix using the following formula (for Pearson correlation):\n",
        "\n",
        "ùëü\n",
        "ùë•\n",
        "ùë¶\n",
        "=\n",
        "Cov\n",
        "(\n",
        "ùëã\n",
        ",\n",
        "ùëå\n",
        ")\n",
        "ùúé\n",
        "ùëã\n",
        "ùúé\n",
        "ùëå\n",
        "r\n",
        "xy\n",
        "‚Äã\n",
        " =\n",
        "œÉ\n",
        "X\n",
        "‚Äã\n",
        " œÉ\n",
        "Y\n",
        "‚Äã\n",
        "\n",
        "Cov(X,Y)\n",
        "‚Äã\n",
        "\n",
        "where\n",
        "Cov\n",
        "(\n",
        "ùëã\n",
        ",\n",
        "ùëå\n",
        ")\n",
        "Cov(X,Y) is the covariance between features\n",
        "ùëã\n",
        "X and\n",
        "ùëå\n",
        "Y, and\n",
        "ùúé\n",
        "ùëã\n",
        "œÉ\n",
        "X\n",
        "‚Äã\n",
        "  and\n",
        "ùúé\n",
        "ùëå\n",
        "œÉ\n",
        "Y\n",
        "‚Äã\n",
        "  are the standard deviations of\n",
        "ùëã\n",
        "X and\n",
        "ùëå\n",
        "Y.\n",
        "\n",
        "Sort the pairs based on the magnitude of the correlation values.\n",
        "\n",
        "Report the highest correlations: You can identify which continuous features have the strongest relationships.\n",
        "\n",
        "Would you like assistance in calculating or visualizing this on your data?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PWKa8mSN_kVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.To determine the optimal number of principal components in your case, follow these steps:\n",
        "\n",
        "1. Variance Explained:\n",
        "Each principal component captures a portion of the total variance in the dataset. The goal is to select enough components to explain a large portion of the variance (e.g., 90%, 95%).\n",
        "2. Steps to Find the Optimal Number:\n",
        "Run PCA: Perform PCA on the dataset and calculate the explained variance ratio for each principal component. This will show how much variance each component explains.\n",
        "\n",
        "Cumulative Variance: Sum the explained variance ratios to calculate the cumulative variance explained by the first, second, third, etc., components.\n",
        "\n",
        "Choose a Threshold: Commonly, you aim to retain 90% to 95% of the variance. The optimal number of components is the number required to exceed this threshold.\n",
        "\n",
        "3. Scree Plot:\n",
        "Plot the explained variance of each principal component in a Scree plot. This plot shows the percentage of variance explained by each component. You can look for the \"elbow point\", where the explained variance starts to level off, indicating that additional components add diminishing amounts of variance.\n",
        "Example:\n",
        "If you're aiming for 90% variance, you can sum the explained variance ratios for each component until the total reaches 90%. The number of components needed to reach that point is the optimal number.\n",
        "Would you like to proceed with this approach on your data? I can guide you through the process or perform a simulation based on some sample values."
      ],
      "metadata": {
        "id": "yVmIFh19_aLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Yes, both PCA (Principal Component Analysis) and t-SNE (t-Distributed Stochastic Neighbor Embedding) are effective tools for visualizing separation between groups (such as the presence and absence of disease) by reducing the dimensionality of the data.\n",
        "\n",
        "PCA:\n",
        "Goal: PCA reduces the dimensionality of data while retaining as much variance as possible. It's a linear transformation that projects the data onto a few principal components that capture the highest variance.\n",
        "Interpretation: If there is a clear separation between the presence and absence of disease, you may observe distinct clusters in the 2D PCA plot (with PC1 and PC2 as axes). However, PCA is linear and may not capture complex, non-linear relationships between the features and disease presence.\n",
        "t-SNE:\n",
        "Goal: t-SNE is a non-linear dimensionality reduction technique that is particularly good at preserving local structure in the data. It emphasizes clustering and separations in high-dimensional data when projected into a lower-dimensional space.\n",
        "Interpretation: A 2D t-SNE plot may reveal more obvious and clearer groupings or clusters, especially when the relationships between features and the presence of disease are non-linear. t-SNE excels at showing separations where PCA might struggle.\n",
        "When applied:\n",
        "PCA would show a clear separation if the features that drive the disease variance are linearly separable in the reduced space. If the clusters overlap in the PCA plot, it may suggest that the disease does not have strong linear boundaries.\n",
        "t-SNE, being more sensitive to local structures and non-linear relationships, often provides better visualization for distinguishing between the presence and absence of disease if the features are not linearly separable. You would expect distinct clusters in a t-SNE plot when there is a clear difference.\n",
        "Summary:\n",
        "If the disease is well-represented by linear patterns in the data, you will see a clear separation in PCA plots.\n",
        "t-SNE may show better separation when the relationships are non-linear or more complex.\n",
        "Would you like help generating PCA and t-SNE plots from your data, or guidance on how to perform these analyses?"
      ],
      "metadata": {
        "id": "HEC2igPp-9Hr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.To identify the three continuous features that show significant statistical differences in distribution between the presence and absence of a disease, you typically use statistical tests to compare the distributions of each feature across the two groups (with and without the disease).\n",
        "\n",
        "Common Steps:\n",
        "Split the data: Separate the data into two groups based on disease presence (positive) and absence (negative).\n",
        "\n",
        "Choose a statistical test:\n",
        "\n",
        "T-test (for normally distributed data): Used to compare the means of the two groups.\n",
        "Mann-Whitney U test (for non-normally distributed data): A non-parametric test that compares the distributions of the two groups.\n",
        "ANOVA (for multiple comparisons): Used when testing more than two groups, but can be applied if comparing many continuous features.\n",
        "Assess statistical significance: You calculate a p-value for each feature to determine whether there is a statistically significant difference between the distributions.\n",
        "\n",
        "If the p-value is less than a chosen significance level (e.g., 0.05), it indicates that the difference is significant.\n",
        "Adjust for multiple comparisons (if necessary): If you test many features, you might apply corrections such as the Bonferroni correction or False Discovery Rate (FDR) to avoid inflating the likelihood of finding false positives.\n",
        "\n",
        "Example Workflow:\n",
        "Perform a T-test or Mann-Whitney U test on each continuous feature.\n",
        "Sort the features based on their p-values.\n",
        "Select the three features with the lowest p-values that are statistically significant (p < 0.05).\n",
        "Would you like assistance with performing these steps using actual data?"
      ],
      "metadata": {
        "id": "sU1SL2vT-1pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JK-m__sH-tfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.The percentage of samples with a disease is calculated using the following formula:\n",
        "\n",
        "Percentage¬†of¬†Samples¬†with¬†Disease\n",
        "=\n",
        "(\n",
        "Number¬†of¬†Samples¬†with¬†Disease\n",
        "Total¬†Number¬†of¬†Samples\n",
        ")\n",
        "√ó\n",
        "100\n",
        "Percentage¬†of¬†Samples¬†with¬†Disease=(\n",
        "Total¬†Number¬†of¬†Samples\n",
        "Number¬†of¬†Samples¬†with¬†Disease\n",
        "‚Äã\n",
        " )√ó100\n",
        "If you provide the number of samples with the disease and the total number of samples, I can calculate the percentage for you.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cFYxEoyE-dNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets move on to do the same analysis on the starbucks nutrition dataset. this dataset contains the nutrition information of starbucks drinks."
      ],
      "metadata": {
        "id": "w8hH8CAhv8tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "upload2 = files.upload()"
      ],
      "metadata": {
        "id": "a_RrGpN1ApCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"star_nutri_expanded.csv\")"
      ],
      "metadata": {
        "id": "NImD-yk0fpmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "YXZJPk4Zf636"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cleaning and filling the missing values in the data"
      ],
      "metadata": {
        "id": "2JA9ZVyywXk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Caffeine (mg)'] = data['Caffeine (mg)'].replace('Varies', np.NaN).replace('varies', np.NaN)\n",
        "data['Caffeine (mg)'] = data['Caffeine (mg)'].astype(np.float64)\n",
        "data['Caffeine (mg)'] = data['Caffeine (mg)'].fillna(data['Caffeine (mg)'].mean())"
      ],
      "metadata": {
        "id": "VrE3QA0GgCTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Total Fat (g)'].unique()"
      ],
      "metadata": {
        "id": "0PNMhN2GiQ6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Total Fat (g)'] = data['Total Fat (g)'].replace('3 2', '3.2')"
      ],
      "metadata": {
        "id": "Suua5EkTiRxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "u-I6TsTLkiMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract columns with int and float types\n",
        "numeric_columns = data.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "\n",
        "# Print the numeric columns\n",
        "print(numeric_columns)\n"
      ],
      "metadata": {
        "id": "xPfMl26LknBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be analysing the dataset using the fact that weather the drink comes under the category tea or not"
      ],
      "metadata": {
        "id": "9FY9pzBNwete"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Beverage_category'].unique()"
      ],
      "metadata": {
        "id": "LVljNoJgg9Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Tea'] = data['Beverage_category'].apply(lambda x: 1 if x == 'Tazo¬Æ Tea Drinks' else 0)\n",
        "data = data.drop('Beverage_category', axis=1)"
      ],
      "metadata": {
        "id": "4UZ4Tk7whVEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  one hot encoding of categorical features in data\n",
        "def onehot_encode(df, columns, prefixes):\n",
        "    df = df.copy()\n",
        "    for column, prefix in zip(columns, prefixes):\n",
        "        dummies = pd.get_dummies(df[column], prefix=prefix)\n",
        "        df = pd.concat([df, dummies], axis=1)\n",
        "        df = df.drop(column, axis=1)\n",
        "    return df"
      ],
      "metadata": {
        "id": "OZanK3hSi6sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = onehot_encode(\n",
        "    data,\n",
        "    columns=['Beverage', 'Beverage_prep'],\n",
        "    prefixes=['bev', 'bevp']\n",
        ")"
      ],
      "metadata": {
        "id": "PlZCj2rwi7us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = data.replace({True: 1, False: 0})\n"
      ],
      "metadata": {
        "id": "w7qKp8JMwysG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.applymap(lambda x: np.float64(str(x).replace('%', '')))"
      ],
      "metadata": {
        "id": "I-RIoXq6ig3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "J_BgqsKogWE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "iB5PmiPojj3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "\n",
        "# Create a pie chart of the 'Tea' column also write your observation form the plot\n",
        "\n"
      ],
      "metadata": {
        "id": "KxZYShn1hpYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# perform pca on the data and plot the explained variace ratio, what is the optimal number of principal components in this case ?\n"
      ],
      "metadata": {
        "id": "HL2dDjYfh2L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# visualise the principal components, choose the number of principal components based on the above plot. What is you observation from the plot?\n"
      ],
      "metadata": {
        "id": "tNKX2JJVjwrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# plot the first 2 components of tsne, whats you observation from the plot?\n"
      ],
      "metadata": {
        "id": "tgpIgiYdlVcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# create a correlation matrix and plot the heatmap, whats your observation from the heatmap ?\n"
      ],
      "metadata": {
        "id": "cOWaPpnmj4OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to do\n",
        "# make a boxplot of all the numeric columns of the dataset. Which column/columns can be the most potential indicator weather its a tea or a non tea drink?\n"
      ],
      "metadata": {
        "id": "VJMRR3yqkNb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To enhance the clarity and professionalism of the provided text, consider the following refined version: In the process of conducting a preliminary Exploratory Data Analysis (EDA), we have utilized various techniques to gain insights into the datasets under consideration. It's important to note that our analysis extends beyond the initial visualizations, embracing a multitude of methods to thoroughly understand the data.\n",
        "Among the array of tools available for EDA, one particularly easy solution is the use of the pandas profiling library. This tool significantly simplifies the process of exploring the fundamental distribution of data within a dataset. By generating detailed profile reports, pandas profiling provides a comprehensive overview of the dataset's characteristics, including but not limited to, the distribution of variables, presence of missing values, and potential correlations between variables.\n",
        "Furthermore, we are utilizing Google Colab notebooks, the integration of AI tools offers an additional avenue for data visualization and analysis. These tools can automatically generate insightful plots and statistics, further enriching the data exploration process."
      ],
      "metadata": {
        "id": "_yRYeXpfzY7O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iDevVx8Bo8lN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}